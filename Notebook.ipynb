{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":82695,"databundleVersionId":9738540,"sourceType":"competition"},{"sourceId":8218776,"sourceType":"datasetVersion","datasetId":4871830},{"sourceId":8897601,"sourceType":"datasetVersion","datasetId":5297895},{"sourceId":9094368,"sourceType":"datasetVersion","datasetId":5251603},{"sourceId":9688062,"sourceType":"datasetVersion","datasetId":5920031},{"sourceId":9734430,"sourceType":"datasetVersion","datasetId":5957531},{"sourceId":9948011,"sourceType":"datasetVersion","datasetId":6117312},{"sourceId":10160879,"sourceType":"datasetVersion","datasetId":4581967},{"sourceId":200567623,"sourceType":"kernelVersion"},{"sourceId":204642927,"sourceType":"kernelVersion"},{"sourceId":208327345,"sourceType":"kernelVersion"},{"sourceId":118192,"sourceType":"modelInstanceVersion","modelInstanceId":99392,"modelId":123481},{"sourceId":139763,"sourceType":"modelInstanceVersion","modelInstanceId":118363,"modelId":127417},{"sourceId":178348,"sourceType":"modelInstanceVersion","modelInstanceId":151944,"modelId":174396}],"dockerImageVersionId":30787,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%time\n\n# Step 1: Uninstall conflicting packages\n!pip uninstall -y torch torchaudio apache-beam accelerate transformers\n\n# Step 2: Install the core dependencies with specific versions\n!pip install -q torch==2.3.1\n!pip install -q torchaudio==2.3.1\n!pip install -q accelerate==0.26.1\n!pip install -q apache-beam==2.46.0\n!pip install -q transformers==4.42.4\n\n# Step 3: Install other packages without conflicts\n!pip install -q --no-index --find-links=/kaggle/input/making-wheels-of-necessary-packages-for-vllm vllm\n!pip install -q -U /kaggle/input/vllm-t4-fix/grpcio-1.62.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n!pip install -q -U /kaggle/input/vllm-t4-fix/ray-2.11.0-cp310-cp310-manylinux2014_x86_64.whl\n!pip install -q --no-deps --no-index /kaggle/input/hf-libraries/sentence-transformers/sentence_transformers-3.1.0-py3-none-any.whl\n!pip install --no-deps --no-index /kaggle/input/logits-processor-zoo/logits_processor_zoo-0.1.0-py3-none-any.whl\n!pip install --no-index /kaggle/input/bitsandbytes0-42-0/bitsandbytes-0.42.0-py3-none-any.whl --find-links=/kaggle/input/bitsandbytes0-42-0\n!pip install --no-index /kaggle/input/bitsandbytes0-42-0/optimum-1.21.2-py3-none-any.whl --find-links=/kaggle/input/bitsandbytes0-42-0\n!pip install --no-index /kaggle/input/bitsandbytes0-42-0/auto_gptq-0.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl --find-links=/kaggle/input/bitsandbytes0-42-0\n\n# Step 4: Verify installations\n!pip list\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T06:08:04.825635Z","iopub.execute_input":"2024-12-11T06:08:04.826036Z"}},"outputs":[{"name":"stdout","text":"Found existing installation: torch 2.4.0+cpu\nUninstalling torch-2.4.0+cpu:\n  Successfully uninstalled torch-2.4.0+cpu\nFound existing installation: torchaudio 2.4.0+cpu\nUninstalling torchaudio-2.4.0+cpu:\n  Successfully uninstalled torchaudio-2.4.0+cpu\nFound existing installation: apache-beam 2.46.0\nUninstalling apache-beam-2.46.0:\n  Successfully uninstalled apache-beam-2.46.0\nFound existing installation: accelerate 0.34.2\nUninstalling accelerate-0.34.2:\n  Successfully uninstalled accelerate-0.34.2\nFound existing installation: transformers 4.45.1\nUninstalling transformers-4.45.1:\n  Successfully uninstalled transformers-4.45.1\n\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x79c4380193f0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/torch/\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# Basic Python Libraries\nimport os\nimport sys\nimport warnings\nimport math\n\n# Data Processing Libraries\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import NearestNeighbors\n\n# PyTorch and Related\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom torch.utils.data import Dataset, DataLoader\n\n# Transformers and PEFT\nfrom transformers import (\n    AutoTokenizer,\n    AutoModel,\n    AutoConfig,\n    AutoModelForMaskedLM,\n    BitsAndBytesConfig\n)\nfrom peft import (\n    LoraConfig,\n    get_peft_model\n)\n\n# Progress Bars\nfrom tqdm import tqdm\nfrom tqdm.autonotebook import trange\n\nwarnings.filterwarnings('ignore')\n\npath_prefix = \"/kaggle/input/eedi-mining-misconceptions-in-mathematics\"\nmodel_path = \"/kaggle/input/sfr-embedding-mistral/SFR-Embedding-2_R\"\nlora_path=\"/kaggle/input/v7-recall/epoch_19_model/adapter.bin\"\ndevice='cuda:0'","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"full_df = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/test.csv\")\n\n\nrows = []\nfor idx, row in full_df.iterrows():\n    for option in [\"A\", \"B\", \"C\", \"D\"]:\n        if option == row.CorrectAnswer:\n            continue\n            \n        correct_answer = row[f\"Answer{row.CorrectAnswer}Text\"]\n\n        query_text =f\"###question###:{row['SubjectName']}-{row['ConstructName']}-{row['QuestionText']}\\n###Correct Answer###:{correct_answer}\\n###Misconcepte Incorrect answer###:{option}.{row[f'Answer{option}Text']}\"\n\n        rows.append({\"query_text\": query_text, \n                     \"QuestionId_Answer\": f\"{row.QuestionId}_{option}\",\n                     \"ConstructName\": row.ConstructName,\n                     \"SubjectName\": row.SubjectName,\n                     \"QuestionText\": row.QuestionText,\n                     \"correct_answer\": correct_answer,\n                     \"incorrect_answer\": row[f\"Answer{option}Text\"]\n                     })\n\ndf = pd.DataFrame(rows)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def batch_to_device(batch, target_device):\n    \"\"\"\n    send a pytorch batch to a device (CPU/GPU)\n    \"\"\"\n    for key in batch:\n        if isinstance(batch[key], Tensor):\n            batch[key] = batch[key].to(target_device)\n    return batch\n\ndef last_token_pool(last_hidden_states: Tensor,\n                    attention_mask: Tensor) -> Tensor:\n    left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\n    if left_padding:\n        return last_hidden_states[:, -1]\n    else:\n        sequence_lengths = attention_mask.sum(dim=1) - 1\n        batch_size = last_hidden_states.shape[0]\n        return last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]\n\ndef get_detailed_instruct(task_description: str, query: str) -> str:\n    return f'Instruct: {task_description}\\nQuery: {query}'\n\ndef inference(df, model, tokenizer, device):\n    batch_size = 16\n    max_length = 512\n    sentences = list(df['query_text'].values)\n\n    all_embeddings = []\n    length_sorted_idx = np.argsort([-len(sen) for sen in sentences])\n    sentences_sorted = [sentences[idx] for idx in length_sorted_idx]\n    for start_index in trange(0, len(sentences), batch_size, desc=\"Batches\", disable=False):\n        sentences_batch = sentences_sorted[start_index: start_index + batch_size]\n        features = tokenizer(sentences_batch, max_length=max_length, padding=True, truncation=True,\n                             return_tensors=\"pt\")\n        features = batch_to_device(features, device)\n        with torch.no_grad():\n            outputs = model.model(**features)\n            embeddings = last_token_pool(outputs.last_hidden_state, features['attention_mask'])\n            embeddings = torch.nn.functional.normalize(embeddings, dim=-1)\n            embeddings = embeddings.detach().cpu().numpy().tolist()\n        all_embeddings.extend(embeddings)\n\n    all_embeddings = [np.array(all_embeddings[idx]).reshape(1, -1) for idx in np.argsort(length_sorted_idx)]\n\n    return np.concatenate(all_embeddings, axis=0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(lora_path.replace(\"/adapter.bin\",\"\"))\nbnb_config = BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_use_double_quant=True,\n            bnb_4bit_quant_type=\"nf4\",\n            bnb_4bit_compute_dtype=torch.bfloat16\n        )\nbackbone = AutoModel.from_pretrained(model_path, quantization_config=bnb_config,device_map=device)\nconfig = LoraConfig(\n        r=64,\n        lora_alpha=256,\n        target_modules=[\n            \"q_proj\",\n            \"k_proj\",\n            \"v_proj\",\n            \"o_proj\",\n            \"gate_proj\",\n            \"up_proj\",\n            \"down_proj\",\n        ],\n        bias=\"none\",\n        lora_dropout=0.05,  # Conventional\n        task_type=\"CAUSAL_LM\",\n    )\nmodel = get_peft_model(backbone, config)\nd = torch.load(lora_path, map_location=model.device)\nmodel.load_state_dict(d, strict=False)\nmodel = model.eval()\nmodel = model.to(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"task_description = 'Given a math question and a misconcepte incorrect answer, please retrieve the most accurate reason for the misconception. You need to thought carefully and reasonal'\n\n\nV_answer = inference(df, model, tokenizer, device)\n\nmisconception_df = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/misconception_mapping.csv\")\nmisconception_df[\"query_text\"] = misconception_df[\"MisconceptionName\"]\n\nV_misconception = inference(misconception_df, model, tokenizer, device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\ndef get_matches(V_topic, V_content, n_neighbors=25):\n    \n    neighbors_model = NearestNeighbors(n_neighbors=n_neighbors, metric='cosine', algorithm=\"brute\", n_jobs=-1)\n    neighbors_model.fit(V_content)\n    dists, indices = neighbors_model.kneighbors(V_topic)\n    \n    return indices\n\nindices = get_matches(V_answer, V_misconception, n_neighbors=25)\nindices.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import gc\n\ndel backbone, model\n\ngc.collect()\ntorch.cuda.empty_cache()\n\nnp.save(\"indices.npy\", indices)\ndf.to_parquet(\"df.parquet\", index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile run_vllm.py\n\nimport vllm\nimport numpy as np\nimport pandas as pd\nfrom transformers import PreTrainedTokenizer, AutoTokenizer\nfrom typing import List\nimport torch\nfrom logits_processor_zoo.vllm import MultipleChoiceLogitsProcessor\nimport re\n\nmodel_path = \"/kaggle/input/m/abdullahmeda/qwen2.5/transformers/math-7b-instruct/1\"\ntokenizer = AutoTokenizer.from_pretrained(model_path)\n\n\ndef preprocess_text(x):\n    x = re.sub(\"http\\w+\", '',x)   # Delete URL\n    x = re.sub(r\"\\.+\", \".\", x)    # Replace consecutive commas and periods with one comma and period character\n    x = re.sub(r\"\\,+\", \",\", x)\n    x = re.sub(r\"\\\\\\(\", \" \", x)\n    x = re.sub(r\"\\\\\\)\", \" \", x)\n    x = re.sub(r\"[ ]{1,}\", \" \", x)\n    x = x.strip()                 # Remove empty characters at the beginning and end\n    return x\n\nPROMPT  = \"\"\"Here is a question about {ConstructName}({SubjectName}).\nQuestion: {Question}\nCorrect Answer: {CorrectAnswer}\nIncorrect Answer: {IncorrectAnswer}\n\nYou are a Mathematics teacher. Your task is to reason and identify the misconception behind the Incorrect Answer with the Question.\nAnswer concisely what misconception it is to lead to getting the incorrect answer.\nPick the correct misconception number from the below:\n\n{Retrival}\n\"\"\"\n# just directly give your answers.\n\ndef apply_template(row, tokenizer):\n    messages = [\n        {\n            \"role\": \"user\", \n            \"content\": preprocess_text(\n                PROMPT.format(\n                    ConstructName=row[\"ConstructName\"],\n                    SubjectName=row[\"SubjectName\"],\n                    Question=row[\"QuestionText\"],\n                    IncorrectAnswer=row[f\"incorrect_answer\"],\n                    CorrectAnswer=row[f\"correct_answer\"],\n                    Retrival=row[f\"retrieval\"]\n                )\n            )\n        }\n    ]\n    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    return text\n\n\nmisconception_df = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/misconception_mapping.csv\")\n\ndf = pd.read_parquet(\"df.parquet\")\nindices = np.load(\"indices.npy\")\n\nllm = vllm.LLM(\n    model_path,\n    # quantization=\"awq\",\n    tensor_parallel_size=2,\n    gpu_memory_utilization=0.90, \n    trust_remote_code=True,\n    dtype=\"half\", \n    enforce_eager=True,\n    max_model_len=4096,\n    disable_log_stats=True\n)\ntokenizer = llm.get_tokenizer()\n\n\ndef get_candidates(c_indices):\n    candidates = []\n\n    mis_names = misconception_df[\"MisconceptionName\"].values\n    for ix in c_indices:\n        c_names = []\n        for i, name in enumerate(mis_names[ix]):\n            c_names.append(f\"{i+1}. {name}\")\n\n        candidates.append(\"\\n\".join(c_names))\n        \n    return candidates\n\nsurvivors = indices[:, -1:]\n\nfor i in range(3):\n    c_indices = np.concatenate([indices[:, -8*(i+1)-1:-8*i-1], survivors], axis=1)\n    \n    df[\"retrieval\"] = get_candidates(c_indices)\n    df[\"text\"] = df.apply(lambda row: apply_template(row, tokenizer), axis=1)\n    \n    print(\"Example:\")\n    print(df[\"text\"].values[0])\n    print()\n    \n    responses = llm.generate(\n        df[\"text\"].values,\n        vllm.SamplingParams(\n            n=1,  # Number of output sequences to return for each prompt.\n            top_k=1,  # Float that controls the cumulative probability of the top tokens to consider.\n            temperature=0,  # randomness of the sampling\n            seed=777, # Seed for reprodicibility\n            skip_special_tokens=False,  # Whether to skip special tokens in the output.\n            max_tokens=1,  # Maximum number of tokens to generate per output sequence.\n            logits_processors=[MultipleChoiceLogitsProcessor(tokenizer, choices=[\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"])]\n        ),\n        use_tqdm=True\n    )\n    \n    responses = [x.outputs[0].text for x in responses]\n    df[\"response\"] = responses\n    \n    \n    llm_choices = df[\"response\"].astype(int).values - 1\n    \n    survivors = np.array([cix[best] for best, cix in zip(llm_choices, c_indices)]).reshape(-1, 1)\n\n\n\nresults = []\n\nfor i in range(indices.shape[0]):\n    ix = indices[i]\n    llm_choice = survivors[i, 0]\n    \n    results.append(\" \".join([str(llm_choice)] + [str(x) for x in ix if x != llm_choice]))\n\n\ndf[\"MisconceptionId\"] = results\ndf.to_csv(\"submission.csv\", columns=[\"QuestionId_Answer\", \"MisconceptionId\"], index=False)\nprint('finish write file')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python run_vllm.py","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pd.read_csv(\"submission.csv\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}